## Tokenization

Dans le cadre de ce projet, nous avons appliqu√© `trois (3) principaux algorithmes` de tokenization.
Chacun avec ses avantages et inconv√©nients, ils nous ont permits d'explorer et de bien conna√Ætre ce qui se cache sous le capot de chaun de ces 3 qu'on explor√©.

Nous vous proposons ci-dessous leur impl√©mentation compl√®te en utilisant la libraire `Spacy` et `Hugging ü§ó`.
Avant de se plonger dans le d√©tail, commen√ßons par comprendre ce quoi la `tokenization`, comment fonctionne-t-il ? A quoi sert ?

??? info "Qu'est-ce que la tokenization ?"

    La tokenization est une m√©thode n√©cessaire permettant de segmenter une phrase en mots et les convertir en nombre √† travers lequels la machine va apprendre.

!!! note "Les langes tchadiennes"

    Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod
    nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor
    massa, nec semper lorem quam in massa.

### Les 3 algo les plus utilis√©s

=== "BPE (Byte Pair Encoding)"

    This is some plain text

=== "SentencePiece"

    * First item
    * Second item
    * Third item

=== "Uni gramm"

    1. First item
    2. Second item
    3. Third item

Ci-dessous les scripts pour chaque algo en R et python

### Code Blocks in Content Tabs

=== "Python"

    ```py
    def main():
        print("Hello world!")

    if __name__ == "__main__":
        main()
    ```

=== "R"

    ```js
    function main() {
        console.log("Hello world!");
    }

    main();
    ```
